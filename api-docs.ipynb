{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyOYNUM0v+WENBzKaGSbDYQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomeiLam/ottomator-agents/blob/main/api-docs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u3OYvvALXrY",
        "outputId": "3f4e8ff8-5ee2-4f6e-ea3c-896828b1cbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m242.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m238.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m315.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m331.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m291.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m278.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m215.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m243.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m277.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m274.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m247.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m221.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m291.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m282.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m227.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m274.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m303.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m260.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m263.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m221.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m302.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m319.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m227.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m324.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m206.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m267.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install faiss-cpu openai tiktoken langchain crewai --upgrade --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pathlib, textwrap, pickle, numpy as np, faiss, tiktoken, openai\n",
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "-3U_9vxPLbmT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass, json, textwrap, pathlib, pickle, numpy as np, faiss, openai\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-3-small\""
      ],
      "metadata": {
        "id": "_I9m4P5vLc6E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib, textwrap\n",
        "FILE = pathlib.Path(\"sikka-apis.json\")\n",
        "root = json.loads(FILE.read_text())\n",
        "\n",
        "def collect_docs(node):\n",
        "    \"\"\"\n",
        "    Depth‑first walk over *any* Postman collection object (dict OR list) and\n",
        "    return a list of {content:str, path:str}.\n",
        "    \"\"\"\n",
        "    stack = [([], node)]\n",
        "    out   = []\n",
        "\n",
        "    while stack:\n",
        "        path, cur = stack.pop()\n",
        "\n",
        "        # ── handle dict nodes ───────────────────────────────────────────────\n",
        "        if isinstance(cur, dict):\n",
        "            name     = cur.get(\"name\") or \"<no‑name>\"\n",
        "            new_path = path + [name]\n",
        "\n",
        "            buckets = []\n",
        "\n",
        "            # 1) plain description\n",
        "            if cur.get(\"description\"):\n",
        "                buckets.append(str(cur[\"description\"]))\n",
        "\n",
        "            # 2) request‑level docs + sample body\n",
        "            req = cur.get(\"request\", {})\n",
        "            if isinstance(req, dict):\n",
        "                if req.get(\"description\"):\n",
        "                    buckets.append(req[\"description\"])\n",
        "                raw = req.get(\"body\", {}).get(\"raw\")\n",
        "                if raw: buckets.append(\"```json\\n\"+raw[:2000]+\"\\n```\")\n",
        "\n",
        "            # 3) response sample bodies\n",
        "            for resp in cur.get(\"response\", []):\n",
        "                body = resp.get(\"body\")\n",
        "                if body:\n",
        "                    buckets.append(\"```json\\n\"+body[:2000]+\"\\n```\")\n",
        "\n",
        "            # 4) event → script → exec (often holds markdown examples)\n",
        "            for ev in cur.get(\"event\", []):\n",
        "                exec_lines = ev.get(\"script\", {}).get(\"exec\") or []\n",
        "                if exec_lines:\n",
        "                    buckets.append(\"\\n\".join(exec_lines)[:2000])\n",
        "\n",
        "            # save doc\n",
        "            if buckets:\n",
        "                out.append({\n",
        "                    \"content\": f\"# {' › '.join(new_path)}\\n\\n\" + \"\\n\\n\".join(buckets),\n",
        "                    \"path\":    \" › \".join(new_path)\n",
        "                })\n",
        "\n",
        "            # push children (if any)\n",
        "            for child in cur.get(\"item\", []):\n",
        "                stack.append((new_path, child))\n",
        "\n",
        "        # ── handle list nodes ───────────────────────────────────────────────\n",
        "        elif isinstance(cur, list):\n",
        "            for itm in cur:\n",
        "                stack.append((path, itm))\n",
        "\n",
        "    return out\n",
        "\n",
        "docs = collect_docs(root)\n",
        "print(\"Total docs captured:\", len(docs))\n",
        "print(\"\\nFirst snippet ↓\\n\", docs[0][\"content\"][:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atx7CSq4LeXi",
        "outputId": "2aad2503-3272-4cd2-d24a-4962b7938974"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs captured: 392\n",
            "\n",
            "First snippet ↓\n",
            " # <no‑name>\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
        "\n",
        "# --- choose ONE of the two splitters -------------------------------------\n",
        "# 1) Recursive (general‑purpose, keeps whole lines / paragraphs together first)\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size      = 3000,     # target chars\n",
        "    chunk_overlap   = 150,      # small overlap improves retrieval recall\n",
        "    separators      = [\"\\n\\n\", \"\\n\", \" \", \"\"],  # try large -> small\n",
        ")\n",
        "\n",
        "# 2) Markdown‑aware (keeps headings and fenced code intact)\n",
        "# splitter = MarkdownTextSplitter(chunk_size=3000, chunk_overlap=150)\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def smart_chunk_docs(docs):\n",
        "    out = []\n",
        "    for d in docs:\n",
        "        pieces = splitter.split_text(d[\"content\"])\n",
        "        meta   = d.get(\"meta\") or {\"path\": d.get(\"path\", \"\")}\n",
        "        for i, part in enumerate(pieces):\n",
        "            out.append({\"content\": part, \"meta\": {**meta, \"chunk\": i}})\n",
        "    return out\n",
        "\n",
        "\n",
        "docs_raw   = collect_docs(root)        # 392 in your case\n",
        "docs_chunk = smart_chunk_docs(docs_raw)\n",
        "print(\"Docs before:\", len(docs_raw))\n",
        "print(\"Docs after :\", len(docs_chunk))\n",
        "print(\"Example meta:\", docs_chunk[0][\"meta\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWGnlDYsLg3L",
        "outputId": "07f1e3d7-a625-418e-d3aa-a39aa1acb195"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docs before: 392\n",
            "Docs after : 517\n",
            "Example meta: {'path': '<no‑name>', 'chunk': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def embed_texts(texts: list[str], model: str = \"text-embedding-3-small\"):\n",
        "    \"\"\"\n",
        "    Returns a list[ list[float] ] of embeddings using the new OpenAI ≥1.0.0 SDK.\n",
        "    \"\"\"\n",
        "    resp = client.embeddings.create(model=model, input=texts)\n",
        "    # resp.data is a list of Embedding objects in **input order**\n",
        "    return [e.embedding for e in resp.data]\n",
        "\n",
        "vecs, BATCH = [], 96\n",
        "for i in range(0, len(docs_raw), BATCH):\n",
        "    batch = [d[\"content\"] for d in docs_raw[i:i+BATCH]]\n",
        "    vecs.extend(embed_texts(batch))\n",
        "\n",
        "vecs = np.asarray(vecs, dtype=\"float32\")\n",
        "index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "index.add(vecs)\n",
        "print(\"FAISS index size:\", index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fku7VWSLiDc",
        "outputId": "f03c891e-4117-4443-f845-37be9a9f40e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index size: 392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_sikka(query, k=4):\n",
        "    q_emb  = embed_texts([query])[0]         # returns 1×embedding\n",
        "    _, ids = index.search(np.array([q_emb], dtype=\"float32\"), k)\n",
        "    context = \"\\n\\n---\\n\\n\".join(docs[i][\"content\"] for i in ids[0])\n",
        "\n",
        "    chat = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\n",
        "             \"content\":\"Answer only from context; say 'Not found' otherwise.\"},\n",
        "            {\"role\":\"user\",\n",
        "             \"content\":f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "        ])\n",
        "    return chat.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "90WpqVXALj7v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template  ─────────────────────────────────────────────\n",
        "\n",
        "OPTIMIZER_TEMPLATE = \"\"\"\n",
        "You are the Query Optimizer for a multi‑agent system that builds full‑stack apps with Sikka APIs.\n",
        "\n",
        "USER_MESSAGE:\n",
        "\\\"\\\"\\\"{user_message}\\\"\\\"\\\"\n",
        "\n",
        "Step 1: Scope\n",
        "• If the message is not about building or integrating Sikka APIs, output exactly:\n",
        "  {{ \"scope_ok\": false, \"reason\": \"short explanation\" }}\n",
        "  and STOP.\n",
        "\n",
        "Step 2: Tech stack\n",
        "• Detect any frontend frameworks/languages (e.g. React, Next.js, TypeScript).\n",
        "• Detect any backend frameworks/languages (e.g. Node.js, Express, Python, Flask).\n",
        "• If none are found, default to “React + JavaScript” front end and “Node.js + Express + JavaScript” back end.\n",
        "\n",
        "Step 3: Rewrite & decompose\n",
        "• Produce one sentence `optimized_query` restating the goal.\n",
        "• Produce **four** prompts under `prompts` with these keys:\n",
        "\n",
        "  1. **api_docs**\n",
        "     – Identify all required Sikka endpoints for `optimized_query`.\n",
        "     – For each endpoint, specify the **full base URL including version** (e.g. `https://api.sikkasoft.com/v4`), the path, and HTTP method.\n",
        "     – Document the authentication flow (request_key lifecycle).\n",
        "     – List required headers and body/query parameters per endpoint.\n",
        "     – Include a sample request and a sample response for each endpoint.\n",
        "\n",
        "  2. **frontend**\n",
        "     – Build the UI using the detected frontend framework.\n",
        "     – Call the endpoints identified in `api_docs`.\n",
        "     – Specify component/file names.\n",
        "     – Handle form state, loading indicators, and error displays.\n",
        "\n",
        "  3. **backend**\n",
        "     – Implement server routes using the detected backend framework.\n",
        "     – Include code to obtain and refresh the request_key.\n",
        "     – Show how to call each Sikka endpoint and return JSON responses.\n",
        "\n",
        "  4. **formatter**\n",
        "     – Assemble **all** outputs into a single Markdown document.\n",
        "     – Include headings for Overview, API Documentation, Frontend Code, and Backend Code.\n",
        "\n",
        "Step 4: Output exactly this JSON (no extra keys):\n",
        "\n",
        "```json\n",
        "{{\n",
        "  \"scope_ok\": true,\n",
        "  \"optimized_query\": \"<one‑sentence restatement>\",\n",
        "  \"tech_stack\": {{\n",
        "    \"frontend\": \"<detected or default stack>\",\n",
        "    \"backend\":  \"<detected or default stack>\"\n",
        "  }},\n",
        "  \"prompts\": {{\n",
        "    \"api_docs\":  \"<instruction containing all required elements>\",\n",
        "    \"frontend\":  \"<instruction containing all required elements>\",\n",
        "    \"backend\":   \"<instruction containing all required elements>\",\n",
        "    \"formatter\": \"<instruction containing all required elements>\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "b9Xn1yBwLkyi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CrewAi Agent\n",
        "from crewai import Agent, Task, Crew\n",
        "import re\n",
        "\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "optimizer_agent = Agent(\n",
        "  role = \"Query Optimizer\",\n",
        "  goal = \"Validate scope and normalise user queries about Sikka APIs\",\n",
        "  backstory = \"Expert in Sikka’s API portfolio and software design.\",\n",
        "  allow_delegation = False,\n",
        "  # verbose = False,\n",
        "  llm = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "def run_optimizer(user_message: str):\n",
        "  \"\"\"Helper to execute the optimizer solo (for testing).\"\"\"\n",
        "  prompt = OPTIMIZER_TEMPLATE.format(user_message=user_message)\n",
        "\n",
        "  task = Task(\n",
        "    agent = optimizer_agent,\n",
        "    description = prompt,\n",
        "    expected_output = \"JSON exactly matching the schema above.\"\n",
        "  )\n",
        "\n",
        "  crew = Crew(agents=[optimizer_agent], tasks=[task], verbose=False)\n",
        "  out    = crew.kickoff()                     # CrewOutput\n",
        "  # 1) Grab the raw string from the first task\n",
        "  raw = out.tasks_output[0].raw\n",
        "\n",
        "  # 2) Strip any ```json fences at start/end\n",
        "  #    This regex removes lines that are exactly ``` or ```json\n",
        "  cleaned = \"\\n\".join(\n",
        "      line for line in raw.splitlines()\n",
        "      if not re.match(r\"^```(?:json)?\\s*$\", line)\n",
        "  ).strip()\n",
        "\n",
        "  # 3) Parse the clean JSON\n",
        "  return json.loads(cleaned)\n",
        "\n",
        "user_message = \"I want a React checkout page that saves a patient's card and runs a $25 sale using Sikka sandbox.\"\n",
        "print(run_optimizer(user_message))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2ILqBcFLlzS",
        "outputId": "fa15cfd3-1bc5-49f5-a275-b488feadbc04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'scope_ok': True, 'optimized_query': \"Create a React checkout page to save a patient's card and process a $25 transaction using Sikka sandbox.\", 'tech_stack': {'frontend': 'React + JavaScript', 'backend': 'Node.js + Express + JavaScript'}, 'prompts': {'api_docs': \"Identify all required Sikka endpoints for creating a React checkout page that saves a patient's card and processes a $25 sale. Document the authentication flow including the request_key lifecycle, list required headers and body/query parameters for each endpoint. Include a sample request and sample response for every endpoint.\", 'frontend': 'Build the UI using React. Include routes/endpoints to call as specified in the api_docs. Specify component/file names to manage form state, handle loading, and errors.', 'backend': 'Implement the server routes using Node.js and Express. Include code to obtain and refresh the request_key. Show the implementation for each Sikka endpoint and ensure that responses are returned in JSON format.', 'formatter': 'Assemble all outputs into a single Markdown document. Include clear headings for Overview, API Documentation, Frontend Code, and Backend Code.'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the API‑Docs agent ────────────────────────────────────────\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# Helper: FAISS retriever ─────────────────────────────────────────\n",
        "def retrieve_api_context(query: str, k: int = 6) -> str:\n",
        "    \"\"\"Return the top‑k passages from your FAISS index for this query.\"\"\"\n",
        "    q_emb  = embed_texts([query])[0]\n",
        "    D, idx = index.search(np.array([q_emb],dtype=\"float32\"), k)\n",
        "    return \"\\n\\n---\\n\\n\".join(docs[i][\"content\"] for i in idx[0])\n",
        "\n",
        "api_doc_agent = Agent(\n",
        "    role = \"API Docs Generator\",\n",
        "    goal = \"Produce detailed API documentation for the endpoints needed\",\n",
        "    backstory = \"Specialist in Sikka API reference docs.\",\n",
        "    allow_delegation = False,\n",
        "    llm = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Connect optimizer → api‑docs with Tasks ────────────────────────\n",
        "\n",
        "# ── 3. Mini‑pipeline: API‑Docs step ────────────────────────────────────\n",
        "def run_api_docs_step(opt_out: dict) -> str:\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. No API docs generated.\"\n",
        "\n",
        "    # 1) Build the literal prompt (only the api_docs instruction + FAISS context)\n",
        "    instr  = opt_out[\"prompts\"][\"api_docs\"]\n",
        "    ctx    = retrieve_api_context(opt_out[\"optimized_query\"])\n",
        "    prompt = f\"{instr}\\n\\nHere is the relevant API context:\\n{ctx}\"\n",
        "\n",
        "    # 2) Single‑task Crew\n",
        "    task = Task(\n",
        "        agent           = api_doc_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = (\n",
        "            \"A Markdown document covering ONLY the Sikka endpoints, \"\n",
        "            \"authentication flow, headers, parameters, and sample calls.\"\n",
        "        )\n",
        "    )\n",
        "    crew_output = Crew(\n",
        "        agents=[api_doc_agent],\n",
        "        tasks=[task],\n",
        "        verbose=False\n",
        "    ).kickoff()\n",
        "\n",
        "    # 3) Extract the raw Markdown string from that one task\n",
        "    #    (TaskOutput.raw holds the LLM's literal response)\n",
        "    api_md = crew_output.tasks_output[0].raw\n",
        "\n",
        "    return api_md\n"
      ],
      "metadata": {
        "id": "g-Csxg0uLsA1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 1) Define the Frontend Code Generator agent ─────────────────────────\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "frontend_agent = Agent(\n",
        "    role             = \"Frontend Code Generator\",\n",
        "    goal             = \"Produce React/Next.js frontend code for the checkout flow\",\n",
        "    backstory        = \"Expert in building modern React applications and UI/UX best practices.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# ── 2) Frontend runner helper ──────────────────────────────────────────\n",
        "def run_frontend_step(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate the React/Next.js frontend code.\n",
        "\n",
        "    Parameters:\n",
        "    - opt_out: dict from run_optimizer(...)\n",
        "    - api_docs_md: Markdown string from run_api_docs_step(...)\n",
        "\n",
        "    Returns:\n",
        "    - Raw code string for the frontend\n",
        "    \"\"\"\n",
        "    # 1) Out‑of‑scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. No frontend code generated.\"\n",
        "\n",
        "    # 2) Build the literal prompt\n",
        "    instr  = opt_out[\"prompts\"][\"frontend\"]\n",
        "    prompt = (\n",
        "        f\"{instr}\\n\\nHere are the API docs you should integrate with:\\n\"\n",
        "        f\"{api_docs_md}\"\n",
        "    )\n",
        "\n",
        "    # 3) Single‑task Crew for frontend code\n",
        "    task = Task(\n",
        "        agent           = frontend_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = (\n",
        "            \"A React (or Next.js) component/file structure & code for the checkout UI, \"\n",
        "            \"handling card input, form state, loading, and error display.\"\n",
        "        )\n",
        "    )\n",
        "    crew_output = Crew(agents=[frontend_agent], tasks=[task], verbose=False).kickoff()\n",
        "\n",
        "    # 4) Return only the raw code\n",
        "    return crew_output.tasks_output[0].raw\n"
      ],
      "metadata": {
        "id": "7nO57VoDLtoe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 1) Define the Backend Code Generator agent ───────────────────────────\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "backend_agent = Agent(\n",
        "    role             = \"Backend Code Generator\",\n",
        "    goal             = \"Produce Node.js/Express backend code for the checkout flow\",\n",
        "    backstory        = \"Expert in building secure and scalable Express APIs.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# ── 2) Backend runner helper ────────────────────────────────────────────\n",
        "def run_backend_step(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate the Node.js + Express backend code.\n",
        "\n",
        "    Parameters:\n",
        "    - opt_out: dict from run_optimizer(...)\n",
        "    - api_docs_md: Markdown string from run_api_docs_step(...)\n",
        "\n",
        "    Returns:\n",
        "    - Raw code string for the backend\n",
        "    \"\"\"\n",
        "    # 1) Out‑of‑scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. No backend code generated.\"\n",
        "\n",
        "    # 2) Build the literal prompt\n",
        "    instr  = opt_out[\"prompts\"][\"backend\"]\n",
        "    prompt = (\n",
        "        f\"{instr}\\n\\nHere are the API docs you should integrate with:\\n\"\n",
        "        f\"{api_docs_md}\"\n",
        "    )\n",
        "\n",
        "    # 3) Single‑task Crew for backend code\n",
        "    task = Task(\n",
        "        agent           = backend_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = (\n",
        "            \"Node.js + Express server code with routes to handle saving cards \"\n",
        "            \"and processing payments using the provided Sikka endpoints.\"\n",
        "        )\n",
        "    )\n",
        "    crew_output = Crew(agents=[backend_agent], tasks=[task], verbose=False).kickoff()\n",
        "\n",
        "    # 4) Return only the raw code\n",
        "    return crew_output.tasks_output[0].raw"
      ],
      "metadata": {
        "id": "U1kkqOVDLu68"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# ── 1) Define the Frontend Evaluator agent ─────────────────────────────\n",
        "frontend_evaluator_agent = Agent(\n",
        "    role             = \"Frontend Code Evaluator\",\n",
        "    goal             = (\n",
        "        \"Inspect the React/Next.js frontend code and report *only* critical \"\n",
        "        \"correctness or security issues in JSON format.\"\n",
        "    ),\n",
        "    backstory        = \"Expert in React best practices, form validation, and front‑end security.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# ── 2) Runner helper for frontend evaluation ─────────────────────────────\n",
        "def run_frontend_eval(opt_out: dict, frontend_code: str) -> dict:\n",
        "    \"\"\"\n",
        "    Analyze frontend code for blocking issues.\n",
        "    Returns a dict with:\n",
        "      {\n",
        "        \"issues\": [\n",
        "          {\"location\":\"frontend\",\"line\":<int>,\"message\":\"<desc>\",\"severity\":\"critical\"},\n",
        "          ...\n",
        "        ],\n",
        "        \"suggestions\": [\n",
        "          \"<actionable improvement>\",\n",
        "          ...\n",
        "        ]\n",
        "      }\n",
        "    \"\"\"\n",
        "    # Out‑of‑scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return {\"issues\": [], \"suggestions\": []}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a strict Bug‑Finder focused ONLY on frontend code.\n",
        "Report *only* critical correctness or security issues in this JSON schema:\n",
        "\n",
        "{{\n",
        "  \"issues\": [\n",
        "    {{\n",
        "      \"location\": \"frontend\",\n",
        "      \"line\": <integer>,\n",
        "      \"message\": \"<description>\",\n",
        "      \"severity\": \"critical\"\n",
        "    }},\n",
        "    ...\n",
        "  ],\n",
        "  \"suggestions\": [\n",
        "    \"<actionable improvement>\",\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Here is the frontend code to evaluate:\n",
        "{frontend_code}\n",
        "\n",
        "Do NOT include any markdown or additional keys—output only the JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "    task = Task(\n",
        "        agent           = frontend_evaluator_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = \"A JSON object with keys 'issues' and 'suggestions' as specified.\"\n",
        "    )\n",
        "    out = Crew(agents=[frontend_evaluator_agent], tasks=[task], verbose=False).kickoff()\n",
        "    raw = out.tasks_output[0].raw\n",
        "\n",
        "    # Strip any ``` fences\n",
        "    cleaned = \"\\n\".join(\n",
        "        line for line in raw.splitlines()\n",
        "        if not re.match(r\"^```(?:\\w+)?\\s*$\", line)\n",
        "    ).strip()\n",
        "\n",
        "    return json.loads(cleaned)\n"
      ],
      "metadata": {
        "id": "jLoK-cpvLwHF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# ── 1) Define the Backend Evaluator agent ──────────────────────────────\n",
        "backend_evaluator_agent = Agent(\n",
        "    role             = \"Backend Code Evaluator\",\n",
        "    goal             = (\n",
        "        \"Inspect the Node.js/Express backend code and report *only* critical \"\n",
        "        \"correctness or security issues in JSON format.\"\n",
        "    ),\n",
        "    backstory        = \"Expert in Express API design, authentication flows, and backend security.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# ── 2) Runner helper for backend evaluation ─────────────────────────────\n",
        "def run_backend_eval(opt_out: dict, backend_code: str) -> dict:\n",
        "    \"\"\"\n",
        "    Analyze backend code for blocking issues.\n",
        "    Returns a dict matching:\n",
        "      {\n",
        "        \"issues\": [\n",
        "          {\"location\":\"backend\",\"line\":<int>,\"message\":\"<desc>\",\"severity\":\"critical\"},\n",
        "          ...\n",
        "        ],\n",
        "        \"suggestions\": [\n",
        "          \"<actionable improvement>\",\n",
        "          ...\n",
        "        ]\n",
        "      }\n",
        "    \"\"\"\n",
        "    # Out‑of‑scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return {\"issues\": [], \"suggestions\": []}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a strict Bug‑Finder focused ONLY on backend code.\n",
        "Report *only* critical correctness or security issues in this JSON schema:\n",
        "\n",
        "{{\n",
        "  \"issues\": [\n",
        "    {{\n",
        "      \"location\": \"backend\",\n",
        "      \"line\": <integer>,\n",
        "      \"message\": \"<description>\",\n",
        "      \"severity\": \"critical\"\n",
        "    }},\n",
        "    ...\n",
        "  ],\n",
        "  \"suggestions\": [\n",
        "    \"<actionable improvement>\",\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Here is the backend code to evaluate:\n",
        "{backend_code}\n",
        "\n",
        "Do NOT include any markdown or additional keys—output only the JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "    task = Task(\n",
        "        agent           = backend_evaluator_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = \"A JSON object with keys 'issues' and 'suggestions' as specified.\"\n",
        "    )\n",
        "    out = Crew(agents=[backend_evaluator_agent], tasks=[task], verbose=False).kickoff()\n",
        "    raw = out.tasks_output[0].raw\n",
        "\n",
        "    # Strip any ``` fences if present\n",
        "    cleaned = \"\\n\".join(\n",
        "        line for line in raw.splitlines()\n",
        "        if not re.match(r\"^```(?:\\w+)?\\s*$\", line)\n",
        "    ).strip()\n",
        "\n",
        "    return json.loads(cleaned)\n"
      ],
      "metadata": {
        "id": "0nawYk5eLxFD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# Define a reusable Code Refinement Agent\n",
        "refine_agent = Agent(\n",
        "    role             = \"Code Refinement Agent\",\n",
        "    goal             = (\n",
        "        \"Incorporate evaluator feedback into an existing code snippet,\\\n",
        "        and return the revised code only.\"\n",
        "    ),\n",
        "    backstory        = \"Expert in iterative code improvement and best practices.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Runner helper for code refinement\n",
        "def run_refine_step(\n",
        "    opt_out: dict,            # Optimizer output dict\n",
        "    code_snippet: str,       # Original code (frontend or backend)\n",
        "    eval_report: dict,       # Evaluation report with issues & suggestions\n",
        "    code_type: str           # \"frontend\" or \"backend\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Refines the given code snippet by applying evaluator feedback.\n",
        "\n",
        "    Returns the refined code as a raw string (no additional text).\n",
        "    \"\"\"\n",
        "    # Out-of-scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return code_snippet\n",
        "\n",
        "    # Prepare feedback list\n",
        "    feedback_lines = [f\"- {s}\" for s in eval_report.get(\"suggestions\", [])]\n",
        "    feedback_text  = \"\\n\".join(feedback_lines)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a Code Refinement Agent. Update the following {code_type} code using the evaluator feedback.\n",
        "\n",
        "Evaluator Report:\n",
        "{json.dumps(eval_report, indent=2)}\n",
        "\n",
        "Original {code_type.capitalize()} Code:\n",
        "```\n",
        "{code_snippet}\n",
        "```\n",
        "\n",
        "Please return **only** the refined {code_type} code, applying all critical fixes. Do not include any explanations.\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Create and run the refinement task\n",
        "    task = Task(\n",
        "        agent           = refine_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = (\n",
        "            f\"The updated {code_type} code snippet, with fixes applied.\"\n",
        "        )\n",
        "    )\n",
        "    crew_output = Crew(agents=[refine_agent], tasks=[task], verbose=False).kickoff()\n",
        "\n",
        "    # Extract the raw code response\n",
        "    raw = crew_output.tasks_output[0].raw\n",
        "    # Strip code fences if present\n",
        "    cleaned = \"\\n\".join(\n",
        "        line for line in raw.splitlines()\n",
        "        if not re.match(r\"^```(?:\\w+)?\\s*$$\", line)\n",
        "    )\n",
        "    return cleaned\n"
      ],
      "metadata": {
        "id": "1RVUKkAZLyBk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def _pipeline_frontend(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Full frontend pipeline: generate → evaluate → refine\n",
        "    Returns the final refined frontend code.\n",
        "    \"\"\"\n",
        "    # 1) Generate initial code\n",
        "    code = run_frontend_step(opt_out, api_docs_md)\n",
        "\n",
        "    # 2) Evaluate it\n",
        "    eval_report = run_frontend_eval(opt_out, code)\n",
        "    print(\"[DEBUG] Frontend eval_report:\", eval_report, flush=True)\n",
        "\n",
        "    # 3) If there are critical issues, refine\n",
        "    if eval_report[\"issues\"]:\n",
        "        code = run_refine_step(opt_out, code, eval_report, code_type=\"frontend\")\n",
        "\n",
        "    return code\n",
        "\n",
        "def _pipeline_backend(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Full backend pipeline: generate → evaluate → refine\n",
        "    Returns the final refined backend code.\n",
        "    \"\"\"\n",
        "    # 1) Generate initial code\n",
        "    code = run_backend_step(opt_out, api_docs_md)\n",
        "\n",
        "    # 2) Evaluate it\n",
        "    eval_report = run_backend_eval(opt_out, code)\n",
        "    print(\"[DEBUG] Backend eval_report:\", eval_report, flush=True)\n",
        "\n",
        "    # 3) If there are critical issues, refine\n",
        "    if eval_report[\"issues\"]:\n",
        "        code = run_refine_step(opt_out, code, eval_report, code_type=\"backend\")\n",
        "\n",
        "    return code\n",
        "\n",
        "def run_frontend_and_backend(opt_out: dict, api_docs_md: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Runs both the frontend and backend full pipelines in parallel.\n",
        "    Returns a tuple (final_frontend_code, final_backend_code).\n",
        "    \"\"\"\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        fut_frontend = executor.submit(_pipeline_frontend, opt_out, api_docs_md)\n",
        "        fut_backend  = executor.submit(_pipeline_backend,  opt_out, api_docs_md)\n",
        "\n",
        "        final_frontend = fut_frontend.result()\n",
        "        final_backend  = fut_backend.result()\n",
        "\n",
        "    return final_frontend, final_backend\n"
      ],
      "metadata": {
        "id": "lpT1UB2yLzGj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# ── 1) Define the Formatter Agent ─────────────────────────────────────────\n",
        "formatter_agent = Agent(\n",
        "    role             = \"Documentation Formatter\",\n",
        "    goal             = (\n",
        "        \"Assemble the optimized query, tech stack, API docs, frontend code, \"\n",
        "        \"and backend code into one coherent Markdown document.\"\n",
        "    ),\n",
        "    backstory        = \"Detail‑oriented technical writer and developer.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# ── 2) Runner helper for the formatter ────────────────────────────────────\n",
        "def run_formatter_step(\n",
        "    opt_out: dict,\n",
        "    api_docs_md: str,\n",
        "    frontend_code: str,\n",
        "    backend_code: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Takes the optimizer output dict plus the generated API docs, frontend code,\n",
        "    and backend code, and returns a single Markdown document.\n",
        "    \"\"\"\n",
        "    # Guard: only format if in‑scope\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. Nothing to format.\"\n",
        "\n",
        "    # Build the literal prompt\n",
        "    prompt = f\"\"\"\n",
        "You are the Documentation Formatter.  Using the pieces below, produce **only**\n",
        "a Markdown document with these top‑level headings in this order:\n",
        "\n",
        "# Overview\n",
        "# API Documentation\n",
        "# Frontend Code\n",
        "# Backend Code\n",
        "\n",
        "Include the optimized query and tech stack under Overview, insert the raw API docs\n",
        "under API Documentation, then wrap the frontend and backend code each in fenced\n",
        "```javascript blocks.\n",
        "\n",
        "### Overview\n",
        "\n",
        "**Task:** {opt_out['optimized_query']}\n",
        "\n",
        "**Tech Stack:**\n",
        "- Frontend: {opt_out['tech_stack']['frontend']}\n",
        "- Backend: {opt_out['tech_stack']['backend']}\n",
        "\n",
        "### API Documentation\n",
        "\n",
        "{api_docs_md}\n",
        "\n",
        "### Frontend Code\n",
        "\n",
        "```javascript\n",
        "{frontend_code}\n",
        "```\n",
        "\n",
        "### Backend Code\n",
        "```\n",
        "{backend_code}\n",
        "```\n",
        "\n",
        "Return only the Markdown—no extra commentary. \"\"\".strip()\n",
        "\n",
        "    # Create and run the formatting task\n",
        "    task = Task(\n",
        "        agent           = formatter_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = \"A single Markdown document with the specified sections.\"\n",
        "    )\n",
        "    crew_output = Crew(\n",
        "        agents=[formatter_agent],\n",
        "        tasks=[task],\n",
        "        verbose=False\n",
        "    ).kickoff()\n",
        "\n",
        "    # Extract the raw Markdown\n",
        "    md = crew_output.tasks_output[0].raw\n",
        "\n",
        "    # Strip any accidental code fences around the entire doc\n",
        "    if md.startswith(\"```\"):\n",
        "        md = \"\\n\".join(line for line in md.splitlines()\n",
        "                      if not re.match(r\"^```\", line)).strip()\n",
        "\n",
        "    return md"
      ],
      "metadata": {
        "id": "AOjIMfm4PO0J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def orchestrate(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    Runs the full Sikka checkout pipeline:\n",
        "      1) Optimize the user query\n",
        "      2) Generate API docs\n",
        "      3) Generate & auto‑fix frontend + backend code\n",
        "      4) Format everything into Markdown\n",
        "\n",
        "    Returns the final Markdown (or an out‑of‑scope notice).\n",
        "    \"\"\"\n",
        "    # 1) Optimize\n",
        "    opt_out = run_optimizer(user_message)\n",
        "    print(\"🔍 Optimizer Output:\", opt_out, \"\\n\" + \"─\" * 60, flush=True)\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return f\"⚠️ Out of scope: {opt_out.get('reason','')}\"\n",
        "\n",
        "    # 2) API docs\n",
        "    api_docs = run_api_docs_step(opt_out)\n",
        "    print(\"📄 API Documentation (snippet):\", api_docs[:300], \"\\n\" + \"─\" * 60, flush=True)\n",
        "\n",
        "    # 3) Generate & refine code\n",
        "    frontend_code, backend_code = run_frontend_and_backend(opt_out, api_docs)\n",
        "    print(\"🚀 Frontend Code (snippet):\", frontend_code[:200], flush=True)\n",
        "    print(\"🔧 Backend  Code (snippet):\", backend_code[:200], \"\\n\" + \"─\" * 60, flush=True)\n",
        "\n",
        "    # 4) Format into Markdown\n",
        "    final_md = run_formatter_step(opt_out, api_docs, frontend_code, backend_code)\n",
        "    print(\"🎉 Final Markdown generated.\", flush=True)\n",
        "\n",
        "    return final_md\n",
        "\n",
        "\n",
        "# ── Example usage ───────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    message = (\n",
        "        \"I want a React checkout page that saves a patient's card \"\n",
        "        \"and runs a $25 sale using Sikka sandbox.\"\n",
        "    )\n",
        "    doc = orchestrate(message)\n",
        "    print(\"\\n=== Final Output ===\\n\")\n",
        "    print(doc)\n"
      ],
      "metadata": {
        "id": "-UrpwDkgL1Kx"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}